"""
Parallel Simulation Runner (CPU)
================================
Author: Marcin Plodzien

This script orchestrates the high-throughput execution of QRC simulations.
It leverages Python's `ProcessPoolExecutor` to distribute simulation batches across CPU cores.

Key Features:
-------------
1.  **CPU Optimization**: Forces JAX to use the CPU backend (`JAX_PLATFORM_NAME='cpu'`) preventing 
    memory competition on GPU during parallel runs.
2.  **Dynamic Configuration**: Loads the experiment configuration module at runtime via string argument.
3.  **Data Persistence**: Collects all results into a single Pandas DataFrame and saves as Pickle.
4.  **Robustness**: Wraps individual worker tasks in try/except blocks to prevent single-failure crashes.

Usage:
------
    python3 00_runner_parallel_CPU.py 01_config_qrc_ladder.py [optional: --serial]

"""

import os
import sys
import time
import pickle
import numpy as np
import pandas as pd
import jax
import jax.numpy as jnp
from concurrent.futures import ProcessPoolExecutor, as_completed
from tqdm import tqdm
import importlib

# Import project modules
import utils.engine as ue
# from sklearn.linear_model import LinearRegression # Removed to avoid conflict with parallel processing

# Force CPU (Safe Mode)
# This prevents JAX from attempting to pre-allocate all GPU memory in each subprocess.
os.environ["JAX_PLATFORM_NAME"] = "cpu"
os.environ["XLA_FLAGS"] = "--xla_force_host_platform_device_count=1"

def load_and_prep_data(filename, n_points, mode='batch_data_input', L=3):
    """
    Loads time-series data and prepares it for injection into the quantum system.
    
    Args:
        filename (str): Path to dataset (e.g., 'santafe.txt').
        n_points (int): Number of time steps to load.
        mode (str): Injection strategy.
            - 'single_data_input': Encodes u(t) identically across all L sites. Shape (T, L).
            - 'batch_data_input': Encodes a sliding window [u(t), u(t-1), ...] across sites.
        L (int): Size of the Input Rail.
        
    Returns:
        jnp.ndarray: Prepared input batch of shape (T, L).
    """
    # Load
    try:
        raw = np.loadtxt(filename)
    except OSError:
        raw = np.loadtxt(f"datasets/{filename}")
        
    # Min-Max Normalization to [0, 1]
    ts = (raw - raw.min()) / (raw.max() - raw.min())
    ts = ts[:n_points]
    
    # Prep
    T = len(ts)
    if mode == 'single_data_input':
        # Broadcast scalar u(t) to vector [u(t), u(t), ..., u(t)]
        # Shape: (T, L)
        inp = np.repeat(ts[:, None], L, axis=1)
        
    elif mode == 'batch_data_input':
        # Sliding Window / Delay Embedding
        # Site 0 gets u(t), Site 1 gets u(t-1), etc.
        padded = np.pad(ts, (L, 0), constant_values=0.0)
        windows = []
        for t in range(T):
            # Window [t+1 : t+L+1] reversed gives [u(t), u(t-1)...]
            win = padded[t+1 : t+L+1][::-1]
            windows.append(win)
        inp = np.array(windows)
        
    return jnp.array(inp)

def worker_fn(config):
    """
    Worker Function for a Single Simulation Batch.
    Executed in a separate process.
    
    Workflow:
    1.  Unpack parameters from `config` dictionary.
    2.  Load Dataset.
    3.  Generate Random Keys (Realizations).
    4.  Call `utils.engine.simulation_kernel` via `jax.vmap` to run N_realizations in parallel (SIMD).
    5.  Collect results and format into a Pandas DataFrame.
    
    Args:
        config (dict): Single configuration dictionary generated by `01_config_*.py`.
        
    Returns:
        dict: {'status': 'success', 'data': DataFrame} or {'status': 'error', 'error': msg}
    """
    try:
        # Unpack Config
        L = config['L']
        t_evol = config['t_evol']
        dt = config['dt']
        h_mag = config['h_mag']
        N_real = config['n_realizations']
        topology = config['ham_config']['topology']
        
        # Data Loading
        d_mode = config['data_input_type']
        target_dataset = config['target_dataset']
        # T_max is fixed to 4000 for standard validation runs
        inp_jax = load_and_prep_data(target_dataset, 4000, d_mode, L)
        
        # Observables
        observables = tuple(config.get('observables', ('Z',)))
        prepare_meas = config.get('prepare_measurement', True)
        
        # Unpack Hamiltonian Parameters (Defaulting to 0 if missing)
        hc = config['ham_config']
        J_rail_left = tuple(hc.get('J_rail_left', (0.0, 0.0, 0.0)))
        J_rail_right = tuple(hc.get('J_rail_right', (0.0, 0.0, 0.0)))
        J_rung = tuple(hc.get('J_rung', (0.0, 0.0, 0.0)))
        J_all = tuple(hc.get('J_all', (0.0, 0.0, 0.0)))
        
        if 'field' in hc:
             fL = fR = tuple(hc['field'])
        else:
             fL = tuple(hc.get('field_L', (0.0, 0.0, 0.0)))
             fR = tuple(hc.get('field_R', (0.0, 0.0, 0.0)))
             
        # Random Keys
        seed_base = config['seed_base']
        real_start = config['realization_start']
        master_key = jax.random.PRNGKey(seed_base + real_start)
        keys = jax.random.split(master_key, N_real)
        
        # JAX VMAP: Vectorize over the 'keys' (axis 0), keep other args fixed (None)
        # This runs multiple realizations simultaneously or loops efficiently.
        # Adjusted in_axes to match 18 arguments (including field_disorder)
        simulation_batch = jax.vmap(ue.simulation_kernel, 
                                    in_axes=(0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None))
                                    
        # Execute JAX Kernel
        # Arguments must match 'simulation_kernel' signature in utils/engine.py
        res_ens = simulation_batch(
            keys, inp_jax, 
            L, t_evol, dt, h_mag, 
            J_rail_left, J_rail_right, J_rung, J_all, 
            fL, fR, 
            config['input_state_type'], topology, 
            config.get('integration_method', 'trotter'), 
            observables, prepare_meas,
            config.get('field_disorder', True)
        )
        
        # Force computation and convert to NumPy
        res_ens.block_until_ready()
        res_np = np.array(res_ens) # Shape: (N_real, T, n_observables)
        
        # Prepare DataFrame Columns
        obs_col_names = []
        for obs_name in observables:
             if 'local' in obs_name:
                 # Check if NN correlation (ZZ, XX, YY) -> Size L-1
                 if any(x in obs_name for x in ['ZZ', 'XX', 'YY']):
                     obs_col_names.extend([f"{obs_name}_{i}" for i in range(L-1)])
                 else:
                     # On-site (Z, X, Y) -> Size L
                     obs_col_names.extend([f"{obs_name}_{i}" for i in range(L)])
             else:
                 # Total/Std usually return 1 value
                 obs_col_names.append(obs_name)
                 
        # Verify shape consistency
        actual_n_obs = res_np.shape[2]
        
        if actual_n_obs == 1:
            print(f"DEBUG: prepare_meas={prepare_meas}. Obs Shape={res_np.shape}. ONLY 1 OBS RETURNED (Likely Norm Error).")
        
        if len(obs_col_names) != actual_n_obs:
            print(f"Warning: Column name mismatch. Expected {len(obs_col_names)}, got {actual_n_obs}. Using generic names.")
            obs_col_names = [f"Obs_{i}" for i in range(actual_n_obs)]
            
        # Metadata Extraction (for DataFrame)
        u_k = inp_jax[:, 0] # Extract input signal for reference
        
        dfs = []
        for r in range(N_real):
            # Create DF for this realization
            df_real = pd.DataFrame(res_np[r], columns=obs_col_names)
            
            # Add Metadata Columns
            df_real['k'] = np.arange(len(df_real))  # Step index
            
            if len(u_k) == len(df_real):
                df_real['s_k'] = np.array(u_k)      # Input value
            else:
                df_real['s_k'] = np.nan
                
            df_real['t_evol'] = t_evol
            df_real['h_mag'] = h_mag
            df_real['N'] = config['N']
            df_real['L'] = L
            df_real['field_disorder'] = config.get('field_disorder', True)
            
            # Config identification
            pnames = config.get('param_names', {})
            df_real['j_rung_name'] = pnames.get('J_rungs', 'Unknown')
            df_real['j_rail_left_name'] = pnames.get('J_rail_left', 'Unknown')
            df_real['j_rail_right_name'] = pnames.get('J_rail_right', 'Unknown')
            
            df_real['config_name'] = config['name'] 
            
            df_real['input_state_type'] = config['input_state_type']
            df_real['data_input_type'] = config['data_input_type']
            df_real['realization'] = real_start + r
            df_real['topology'] = topology
            df_real['dt'] = dt
            
            dfs.append(df_real)
            
        full_df = pd.concat(dfs, ignore_index=True)
        
        # --- SAVE INDIVIDUAL PICKLE ---
        # User requested separate files per configuration
        # NEW: Save to 'data' subfolder
        batch_dir = f"results/{config.get('config_name_batch', 'UnknownConfig')}/data"
        os.makedirs(batch_dir, exist_ok=True)
        
        # New Standardized Filename
        import utils.helpers as uh
        filename = uh.generate_result_filename(config, iter_idx=real_start)
        pkl_path = f"{batch_dir}/{filename}"
        
        full_df.to_pickle(pkl_path)
        # -----------------------------
        
        return {'status': 'success', 'data': full_df, 'path': pkl_path}
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return {'status': 'error', 'error': str(e)}

def main():
    start_time = time.time()
    
    # Argument Parsing
    config_name = sys.argv[1] if len(sys.argv) > 1 else "01_config_qrc_ladder.py"

    if config_name.endswith('.py'):
        config_name = config_name[:-3]
    try:
        config_module = importlib.import_module(config_name)
    except ModuleNotFoundError:
        print(f"Config '{config_name}' not found.")
        return

    print(f"Loading Configuration: {config_name}...")
    tasks = config_module.generate_configs()
    print(f"Generated {len(tasks)} batches.")
    
    # Determine Execution Mode
    max_workers = getattr(config_module, 'MAX_WORKERS', 1)
    
    results_list = []
    
    if max_workers > 1:
        print(f"Running PARALLEL with {max_workers} workers.")
        # Use 'spawn' for JAX compatibility
        import multiprocessing
        ctx = multiprocessing.get_context('spawn')
        with ProcessPoolExecutor(max_workers=max_workers, mp_context=ctx) as executor:
            futures = [executor.submit(worker_fn, t) for t in tasks]
            for fut in tqdm(as_completed(futures), total=len(futures)):
                res = fut.result()
                if res['status'] == 'success':
                    results_list.append(res['data'])
                else:
                    print(f"Error: {res['error']}")
    else:
        print("Running SERIAL (Debugging Mode).")
        for t in tqdm(tasks):
             res = worker_fn(t)
             if res['status'] == 'success':
                 results_list.append(res['data'])
             else:
                 print(f"Error: {res['error']}")
                 
    # Merge & Save
    if results_list:
        print("Merging DataFrames...")
        final_df = pd.concat(results_list, ignore_index=True)
        
        # NEW: Consolidated file also goes to data subfolder
        out_dir = f"results/{config_module.CONFIG_NAME}/data"
        os.makedirs(out_dir, exist_ok=True)
        out_path = f"{out_dir}/collected_simulation_results.pkl"
        
        print(f"Saving to {out_path}...")
        final_df.to_pickle(out_path)
        print("Done.")
    else:
        print("No results collected (Sim Failed?).")

    total_time = time.time() - start_time
    hours = total_time // 3600
    minutes = (total_time % 3600) // 60
    seconds = total_time % 60
    print(f"\n--- Execution Completed in {total_time:.2f} seconds ({int(hours)}h {int(minutes)}m {seconds:.2f}s) ---")

if __name__ == "__main__":
    main()
